{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo webcam setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOv10\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "root_path = \"\" # deinfe your working directory path\n",
    "\n",
    "img_path = os.path.join(root_path ,\"yolov10_finetune/fine_tune_kidney_stone_data/images/Modi_with_Gary.png\")\n",
    "\n",
    "pretrained_checkpoint_path = os.path.join(root_path , \"yolov10_finetune/fine_tune_kidney_stone_data/pretrained_checkpoints/yolov10m.pt\")\n",
    "model = YOLOv10(pretrained_checkpoint_path)\n",
    "\n",
    "result = model.predict(img_path)\n",
    "\n",
    "plt.imshow(result[0].plot())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obtain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in fish-species-combined-3 to yolov9:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80619/80619 [00:03<00:00, 25619.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to fish-species-combined-3 in yolov9:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3874/3874 [00:01<00:00, 2977.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"pN9ly9A9Dyo3OEf45Vk6\")\n",
    "project = rf.workspace(\"gemini-fincq\").project(\"fish-species-combined\")\n",
    "version = project.version(3)\n",
    "dataset = version.download(\"yolov9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {HOME}/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P {HOME}/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10s.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10m.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10b.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10x.pt\n",
    "!wget -P {HOME}/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10l.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 876760\n",
      "-rw-r--r--  1 jj  staff    79M May 23 02:38 yolov10b.pt\n",
      "-rw-r--r--  1 jj  staff   100M May 23 02:38 yolov10l.pt\n",
      "-rw-r--r--  1 jj  staff    64M May 23 02:38 yolov10m.pt\n",
      "-rw-r--r--  1 jj  staff    11M May 23 02:38 yolov10n.pt\n",
      "-rw-r--r--  1 jj  staff    31M May 23 02:38 yolov10s.pt\n",
      "-rw-r--r--  1 jj  staff   122M May 23 02:38 yolov10x.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {HOME}/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLOv10\n",
    "def useModel(home=HOME, file_location='test/images.jpeg'):\n",
    "    file_location = f'{home}/{file_location}'\n",
    "    model = YOLOv10(f'{home}/weights/yolov10n.pt')\n",
    "    results = model(source=file_location, conf=0.25)\n",
    "    print(results[0].boxes.xyxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/jj/projects/hackathon/reelrecon-ai/test/images.jpeg: 640x640 (no detections), 95.7ms\n",
      "Speed: 4.9ms preprocess, 95.7ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "tensor([], size=(0, 4))\n"
     ]
    }
   ],
   "source": [
    "useModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/jj/projects/hackathon/reelrecon-ai/test/dog.jpeg: 640x384 1 0, 1 16, 62.9ms\n",
      "Speed: 1.1ms preprocess, 62.9ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "tensor([[6.7041e+01, 2.5264e+02, 6.4549e+02, 9.6621e+02],\n",
      "        [7.4646e-01, 3.4156e+02, 5.4170e+02, 1.2773e+03]])\n"
     ]
    }
   ],
   "source": [
    "useModel(file_location='test/dog.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize prections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "annotated_image = bounding_box_annotator.annotate(\n",
    "\tscene=image, detections=detections)\n",
    "annotated_image = label_annotator.annotate(\n",
    "\tscene=annotated_image, detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as seen above\n",
    "\n",
    "```\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"pN9ly9A9Dyo3OEf45Vk6\")\n",
    "project = rf.workspace(\"gemini-fincq\").project(\"fish-species-combined\")\n",
    "version = project.version(3)\n",
    "dataset = version.download(\"yolov9\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train YOLOv10 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jj/projects/hackathon/reelrecon-ai/fish-species-combined-3\n"
     ]
    }
   ],
   "source": [
    "print(dataset.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jj/projects/hackathon/reelrecon-ai/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:733: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n",
      "New https://pypi.org/project/ultralytics/8.3.34 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.12.7 torch-2.5.1 CPU (Apple M2 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/Users/jj/projects/hackathon/reelrecon-ai/weights/yolov10n.pt, data=/Users/jj/projects/hackathon/reelrecon-ai/fish-species-combined-3/data.yaml, epochs=25, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, val_period=1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/jj/projects/hackathon/reelrecon-ai/runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=23\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1      9856  ultralytics.nn.modules.block.SCDown          [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1     36096  ultralytics.nn.modules.block.SCDown          [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.PSA             [256, 256]                    \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 20                  -1  1     18048  ultralytics.nn.modules.block.SCDown          [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    282624  ultralytics.nn.modules.block.C2fCIB          [384, 256, 1, True, True]     \n",
      " 23        [16, 19, 22]  1    870298  ultralytics.nn.modules.head.v10Detect        [23, [64, 128, 256]]          \n",
      "YOLOv10n summary: 385 layers, 2716010 parameters, 2715994 gradients, 8.4 GFLOPs\n",
      "\n",
      "Transferred 493/595 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "/Users/jj/projects/hackathon/reelrecon-ai/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:276: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/jj/projects/hackathon/reelrecon-ai/fish-species-combined-\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/jj/projects/hackathon/reelrecon-ai/fish-species-combined-3/train/labels.cache\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 132, len(boxes) = 1659. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/jj/projects/hackathon/reelrecon-ai/fish-species-combined-3/\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/jj/projects/hackathon/reelrecon-ai/fish-species-combined-3/valid/labels.cache\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 33, len(boxes) = 381. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "Plotting labels to /Users/jj/projects/hackathon/reelrecon-ai/runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00037, momentum=0.9) with parameter groups 95 weight(decay=0.0), 108 weight(decay=0.0005), 107 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/jj/projects/hackathon/reelrecon-ai/runs/detect/train\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n",
      "       1/25         0G      1.239      4.212      1.763      1.076      6.915   "
     ]
    }
   ],
   "source": [
    "# %cd {HOME}\n",
    "!yolo task=detect mode=train epochs=25 batch=32 plots=True \\\n",
    "model={HOME}/weights/yolov10n.pt \\\n",
    "data={dataset.location}/data.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
